{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - also create labels to be int from 0 to 1 for each class and somehow randomize the data and then split it between data and labels \n",
    "def load_data(base_path: Path, class_names: list, train: bool, resolution: tuple, randomized: bool, flag: str = \"RGB\"):\n",
    "    data = []\n",
    "    labels = []\n",
    "    class_maping = {k:i for i,k in enumerate(class_names)}\n",
    "    num_images_per_class = 1000 if train else 150\n",
    "    train = \"train\" if train else \"test\"\n",
    "    print(f\"Loading each class for {train}.\")\n",
    "    for class_name in class_names:\n",
    "        class_folder_path = base_path/train/class_name\n",
    "        for image_name in tqdm(os.listdir(class_folder_path)[0:num_images_per_class]):\n",
    "            \n",
    "            img = np.array(Image.open(class_folder_path/image_name).convert(flag).resize(resolution))\n",
    "\n",
    "            img_label = class_maping[class_name]\n",
    "            if img is None:\n",
    "                    print(f'This image is bad: {class_folder_path/image_name}')\n",
    "            else:\n",
    "                labels.append(img_label)\n",
    "                data.append(img)\n",
    "    \n",
    "    if randomized:\n",
    "        randomized_indices = np.random.permutation(len(labels))\n",
    "        data, labels = np.array(data)[randomized_indices], np.array(labels)[randomized_indices]\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading each class for train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 828/828 [00:03<00:00, 222.53it/s]\n",
      "100%|██████████| 505/505 [00:01<00:00, 365.31it/s]\n",
      "100%|██████████| 1000/1000 [00:04<00:00, 227.89it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 186.12it/s]\n",
      "100%|██████████| 589/589 [00:03<00:00, 149.42it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 264.00it/s]\n",
      "100%|██████████| 405/405 [00:00<00:00, 422.63it/s]\n",
      "100%|██████████| 1000/1000 [00:05<00:00, 176.85it/s]\n",
      "100%|██████████| 998/998 [00:08<00:00, 117.79it/s]\n",
      "100%|██████████| 1000/1000 [00:03<00:00, 264.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading each class for test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:01<00:00, 100.44it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 227.99it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 229.94it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 419.73it/s]\n",
      "100%|██████████| 69/69 [00:00<00:00, 431.78it/s]\n",
      "100%|██████████| 142/142 [00:00<00:00, 356.80it/s]\n",
      "100%|██████████| 70/70 [00:00<00:00, 378.99it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 577.05it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 185.46it/s]\n",
      "100%|██████████| 150/150 [00:00<00:00, 520.40it/s]\n"
     ]
    }
   ],
   "source": [
    "base_path = Path(\"Z:/Master I/PML - Practical Machine Learning/Unsupervised_Comparison/data/Architectural_Heritage_Elements\")\n",
    "\n",
    "class_names = ['altar', 'apse', 'bell_tower', 'column','dome(inner)','dome(outer)',\n",
    "               'flying_buttress','gargoyle','stained_glass','vault']\n",
    "\n",
    "# class_names = ['altar', 'column','dome(outer)','gargoyle','stained_glass']\n",
    "\n",
    "train_data, train_labels = load_data(base_path, class_names, train = True, resolution=(64,64), randomized = True, flag = 'RGB')\n",
    "# display_some_images(train_data, train_labels)\n",
    "\n",
    "test_data, test_labels = load_data(base_path, class_names, train = False, resolution = (64,64), randomized = True, flag = 'RGB')\n",
    "# display_some_images(test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1221, 64, 64, 3)\n",
      "(1221,)\n",
      "(6399, 64, 64, 3)\n",
      "(6399,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# train_data = train_data.reshape(6399,28,28,1)\n",
    "# test_data = test_data.reshape(1221,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_flatten(data: np.array):\n",
    "    normalized = data / 255.0\n",
    "    # mean = np.mean(arr, axis = 0)\n",
    "    preproc_data = normalized.reshape(len(normalized), -1 )\n",
    "    return preproc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = normalize_and_flatten(test_data)\n",
    "train_features = normalize_and_flatten(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "  def __init__(self, model):\n",
    "    super(FeatureExtractor, self).__init__()\n",
    "\n",
    "    self.features = list(model.features)\n",
    "    self.features = nn.Sequential(*self.features)\n",
    "    self.pooling = model.avgpool\n",
    "    self.flatten = nn.Flatten()\n",
    "    self.fc = model.classifier[0]\n",
    "  \n",
    "  def forward(self, x):\n",
    "    out = self.features(x)\n",
    "    out = self.pooling(out)\n",
    "    out = self.flatten(out)\n",
    "    out = self.fc(out) \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, _transform, device):\n",
    "    model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    new_model = FeatureExtractor(model)\n",
    "    new_model = new_model.to(device)\n",
    "    features = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        transformed_img = _transform(data[i])\n",
    "        img = transformed_img.reshape(1, 3, 64, 64)\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            feature = new_model(img)\n",
    "        features.append(feature.cpu().detach().numpy().reshape(-1))\n",
    "\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "z:\\anaconda3\\envs\\PML_Project2\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "z:\\anaconda3\\envs\\PML_Project2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 8325/8325 [01:53<00:00, 73.13it/s]\n",
      "100%|██████████| 1221/1221 [00:17<00:00, 70.78it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.ToPILImage(),\n",
    "  transforms.Resize(64),\n",
    "  transforms.ToTensor() # this normalizes the data too                       \n",
    "])\n",
    "\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "new_model = FeatureExtractor(model)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "train_features = extract_features(train_data, transform, device)\n",
    "test_features = extract_features(test_data, transform, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1221, 64, 64, 3)\n",
      "(1221,)\n",
      "(1221, 4096)\n",
      "(8325, 64, 64, 3)\n",
      "(8325,)\n",
      "(8325, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "test_features = np.array(test_features)\n",
    "print(test_features.shape)\n",
    "\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "train_features = np.array(train_features)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/saved_features/train_features_64_64.npy\",train_features)\n",
    "np.save(\"./data/saved_features/test_features_64_64.npy\",test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_mapping(cluster_labels,y_train):\n",
    "    \"\"\"\n",
    "    Associates most probable label with each cluster in KMeans model\n",
    "    returns: dictionary of clusters assigned to each label\n",
    "    \"\"\"\n",
    "    labels_mapping = {}\n",
    "    nr_of_clusters = len(np.unique(cluster_labels))\n",
    "    for i in range(nr_of_clusters):\n",
    "        indexes = np.array([1 if cluster_labels[ind]==i else 0 for ind in range(len(cluster_labels))]) # lista noua cu 1 daca valoarea din cluster e egala cu valoarea lui i si 0 altfel \n",
    "        num = np.bincount(y_train[indexes==1]).argmax()\n",
    "        labels_mapping[i] = num\n",
    "\n",
    "    return labels_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "clusters\tinertia\t\thomo\tsilhouette\ttrain_acc\ttest_acc\n",
      "10\t\t272721472.000\t0.368\t0.055\t\t0.533\t\t0.528\n",
      "15\t\t254222112.000\t0.438\t0.045\t\t0.576\t\t0.541\n",
      "20\t\t242452624.000\t0.460\t0.049\t\t0.566\t\t0.532\n",
      "25\t\t233697168.000\t0.485\t0.041\t\t0.594\t\t0.565\n",
      "50\t\t209469504.000\t0.543\t0.028\t\t0.648\t\t0.609\n",
      "80\t\t195258304.000\t0.581\t0.030\t\t0.678\t\t0.622\n",
      "160\t\t174850864.000\t0.621\t0.027\t\t0.699\t\t0.645\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"_\")\n",
    "print(\"clusters\\tinertia\\t\\thomo\\tsilhouette\\ttrain_acc\\ttest_acc\")\n",
    "for i in [10,15,20,25,50,80,160]:\n",
    "    # print(\"Number of clusters: {}\".format(i))\n",
    "    kmeans = KMeans(init=\"k-means++\", n_clusters=i, n_init=4)\n",
    "    kmeans.fit(train_features)\n",
    "\n",
    "    _metrics = [i,\n",
    "                kmeans.inertia_,\n",
    "                metrics.homogeneity_score(train_labels,kmeans.labels_),\n",
    "                metrics.silhouette_score(train_features,kmeans.labels_,metric=\"euclidean\")]\n",
    "\n",
    "    labels_mapping = get_labels_mapping(kmeans.labels_,train_labels)\n",
    "\n",
    "\n",
    "    number_labels_train = np.zeros(len(kmeans.labels_))\n",
    "    for i in range(len(kmeans.labels_)):\n",
    "        number_labels_train[i] = labels_mapping[kmeans.labels_[i]]\n",
    "    _metrics.append(accuracy_score(number_labels_train,train_labels))\n",
    "\n",
    "\n",
    "    predicted = kmeans.predict(test_features)\n",
    "    number_labels_test = np.zeros(len(predicted))\n",
    "    for i in range(len(predicted)):\n",
    "        number_labels_test[i] = labels_mapping[predicted[i]]\n",
    "    _metrics.append(accuracy_score(number_labels_test,test_labels))\n",
    "\n",
    "    formatter_result = (\"{}\\t\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\")\n",
    "\n",
    "    print(formatter_result.format(*_metrics))\n",
    "print(80 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_k_means(kmeans, name, data, labels):\n",
    "\n",
    "    starting_time = time()\n",
    "    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)\n",
    "    fit_time = time() - starting_time\n",
    "\n",
    "    _metrics = [name,\n",
    "                fit_time,\n",
    "                estimator[-1].inertia_,\n",
    "                metrics.homogeneity_score(labels, estimator[-1].labels_),\n",
    "                metrics.silhouette_score(data,\n",
    "                                         estimator[-1].labels_,\n",
    "                                         metric=\"euclidean\",\n",
    "                                         sample_size=300,)]\n",
    "\n",
    "    # Print the results\n",
    "    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.3f}\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_result.format(*_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________\n",
      "init\t\ttime\tinertia\t\thomo\tsilhouette\n",
      "k-means++\t22.698s\t20177540.000\t0.486\t0.046\n",
      "random   \t7.690s\t20164680.000\t0.487\t0.028\n",
      "PCA-based\t2.560s\t20156484.000\t0.492\t0.032\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(60 * \"_\")\n",
    "print(\"init\\t\\ttime\\tinertia\\t\\thomo\\tsilhouette\")\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=25, n_init=4, random_state=0)\n",
    "tune_k_means(kmeans=kmeans, name=\"k-means++\", data=train_features, labels=train_labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=25, n_init=4, random_state=0)\n",
    "tune_k_means(kmeans=kmeans, name=\"random\", data=train_features, labels=train_labels)\n",
    "\n",
    "pca = PCA(n_components=25).fit(train_features)\n",
    "kmeans = KMeans(init=pca.components_, n_clusters=25, n_init=1)\n",
    "tune_k_means(kmeans=kmeans, name=\"PCA-based\", data=train_features, labels=train_labels)\n",
    "\n",
    "print(60  * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_k_means(kmeans, name, data, labels):\n",
    "\n",
    "    starting_time = time()\n",
    "    estimator =  kmeans.fit(data)\n",
    "    fit_time = time() - starting_time\n",
    "\n",
    "    _metrics = [name,\n",
    "                fit_time,\n",
    "                kmeans.inertia_,\n",
    "                metrics.homogeneity_score(labels, kmeans.labels_),\n",
    "                metrics.silhouette_score(data,\n",
    "                                         kmeans.labels_,\n",
    "                                         metric=\"euclidean\",\n",
    "                                         sample_size=300,)]\n",
    "\n",
    "    labels_mapping = get_labels_mapping(kmeans.labels_,train_labels)\n",
    "\n",
    "\n",
    "    number_labels_train = np.zeros(len(kmeans.labels_))\n",
    "    for i in range(len(kmeans.labels_)):\n",
    "        number_labels_train[i] = labels_mapping[kmeans.labels_[i]]\n",
    "    _metrics.append(accuracy_score(number_labels_train,train_labels))\n",
    "\n",
    "    predicted = kmeans.predict(test_features)\n",
    "    number_labels_test = np.zeros(len(predicted))\n",
    "    for i in range(len(predicted)):\n",
    "        number_labels_test[i] = labels_mapping[predicted[i]]\n",
    "    _metrics.append(accuracy_score(number_labels_test,test_labels))\n",
    "\n",
    "    formatter_result = (\"{:.1}\\t\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\")\n",
    "    \n",
    "    # Print the results\n",
    "    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t\\t{:.3f}\\t\\t{:.3f}\")\n",
    "    print(formatter_result.format(*_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________\n",
      "algorithm\ttime\tinertia\t\thomo\tsilhouette\tacc_train\tacc_test\n",
      "lloyd    \t24.657s\t233758128.000\t0.481\t0.034\t\t0.587\t\t0.545\n",
      "elkan    \t21.489s\t233758144.000\t0.481\t0.026\t\t0.587\t\t0.545\n",
      "__________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(90 * \"_\")\n",
    "print(\"algorithm\\ttime\\tinertia\\t\\thomo\\tsilhouette\\tacc_train\\tacc_test\")\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=25, n_init=4, random_state=0, algorithm=\"lloyd\")\n",
    "tune_k_means(kmeans=kmeans, name=\"lloyd\", data=train_features, labels=train_labels)\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=25, n_init=4, random_state=0,algorithm=\"elkan\")\n",
    "tune_k_means(kmeans=kmeans, name=\"elkan\", data=train_features, labels=train_labels)\n",
    "\n",
    "\n",
    "print(90 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not working \n",
    "parameters = {'init':['k-means++', 'random'], 'n_init':[\"auto\", 4 , 10]}\n",
    "\n",
    "kmeans_pipe = Pipeline([('knn', KMeans(n_clusters=25))])\n",
    "\n",
    "def silhouette_score(estimator, X):\n",
    "    clusters = estimator.fit_predict(X)\n",
    "    score = metrics.silhouette_score(train_features, clusters, metric='precomputed')\n",
    "    return score\n",
    "\n",
    "N = len(train_features)\n",
    "cv_custom = [(range(0,N))]\n",
    "\n",
    "clf = GridSearchCV(kmeans_pipe,\n",
    "                   parameters,\n",
    "                   verbose=3,\n",
    "                   scoring=silhouette_score,\n",
    "                   cv=cv_custom\n",
    "                   )\n",
    "\n",
    "clf.fit(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_db_scan(dbscan, value, data, labels):\n",
    "\n",
    "    starting_time = time()\n",
    "    estimator = make_pipeline(StandardScaler(), dbscan).fit(data)\n",
    "    fit_time = time() - starting_time\n",
    "\n",
    "    # core_sample_mask = np.zeros_like(estimator[-1].labels_,dtype=bool)\n",
    "    # core_sample_mask[estimator[-1].core_sample_indices] = True\n",
    "\n",
    "    _metrics = [value,\n",
    "                fit_time,\n",
    "                metrics.silhouette_score(data,\n",
    "                                         estimator[-1].labels_),\n",
    "                metrics.homogeneity_score(labels, estimator[-1].labels_),\n",
    "                metrics.completeness_score(labels, estimator[-1].labels_),\n",
    "                len(np.unique(dbscan.labels_))]\n",
    "\n",
    "    # Print the results\n",
    "    formatter_result = (\"{}\\t\\t{:.3f}\\t{:.3f}\\t\\t{:.3f}\\t{:.3f}\\t{}\")\n",
    "    print(formatter_result.format(*_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "eps_val\t\ttime\tsilhouette\thomo\tcomplet\tnr_clusters\n",
      "10\t\t8.420\t-0.014\t\t0.000\t0.238\t2\n",
      "20\t\t7.597\t-0.201\t\t0.005\t0.252\t12\n",
      "30\t\t7.603\t-0.307\t\t0.034\t0.140\t45\n",
      "40\t\t7.354\t-0.135\t\t0.042\t0.118\t27\n",
      "60\t\t7.871\t0.264\t\t0.011\t0.112\t5\n",
      "80\t\t9.779\t0.395\t\t0.001\t0.097\t3\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"_\")\n",
    "print(\"eps_val\\t\\ttime\\tsilhouette\\thomo\\tcomplet\\tnr_clusters\")\n",
    "\n",
    "for i in [10,20,30,40,60,80]:\n",
    "    db = DBSCAN(eps=i, min_samples=3)\n",
    "    tune_db_scan(dbscan=db, value=i, data=train_features, labels=train_labels)\n",
    "\n",
    "print(80 * \"_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "min_samples\ttime\tsilhouette\thomo\tcomplet\tnr_clusters\n",
      "2\t\t7.815\t-0.267\t\t0.068\t0.148\t143\n",
      "3\t\t7.899\t-0.135\t\t0.042\t0.118\t27\n",
      "4\t\t7.125\t-0.057\t\t0.035\t0.106\t11\n",
      "5\t\t7.620\t0.034\t\t0.031\t0.098\t6\n",
      "6\t\t7.477\t0.023\t\t0.032\t0.100\t6\n",
      "7\t\t7.844\t0.053\t\t0.031\t0.098\t4\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"_\")\n",
    "print(\"min_samples\\ttime\\tsilhouette\\thomo\\tcomplet\\tnr_clusters\")\n",
    "\n",
    "for i in [2,3,4,5,6,7]:\n",
    "    db = DBSCAN(eps=40, min_samples=i)\n",
    "    tune_db_scan(dbscan=db, value=i, data=train_features, labels=train_labels)\n",
    "\n",
    "print(80 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "min_samples\ttime\tsilhouette\thomo\tcomplet\tnr_clusters\n",
      "2\t\t7.994\t0.149\t\t0.014\t0.125\t19\n",
      "3\t\t7.874\t0.264\t\t0.011\t0.112\t5\n",
      "4\t\t7.518\t0.098\t\t0.012\t0.117\t4\n",
      "5\t\t7.673\t0.112\t\t0.012\t0.119\t3\n",
      "6\t\t8.252\t0.291\t\t0.012\t0.119\t2\n",
      "7\t\t7.846\t0.291\t\t0.013\t0.125\t2\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"_\")\n",
    "print(\"min_samples\\ttime\\tsilhouette\\thomo\\tcomplet\\tnr_clusters\")\n",
    "\n",
    "for i in [2,3,4,5,6,7]:\n",
    "    db = DBSCAN(eps=60, min_samples=i)\n",
    "    tune_db_scan(dbscan=db, value=i, data=train_features, labels=train_labels)\n",
    "\n",
    "print(80 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "min_samples\ttime\tsilhouette\thomo\tcomplet\tnr_clusters\n",
      "2\t\t8.929\t0.368\t\t0.002\t0.110\t5\n",
      "3\t\t8.719\t0.395\t\t0.001\t0.097\t3\n",
      "4\t\t8.641\t0.409\t\t0.001\t0.087\t2\n",
      "5\t\t8.845\t0.409\t\t0.001\t0.087\t2\n",
      "6\t\t9.110\t0.409\t\t0.001\t0.087\t2\n",
      "7\t\t9.075\t0.409\t\t0.001\t0.087\t2\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"_\")\n",
    "print(\"min_samples\\ttime\\tsilhouette\\thomo\\tcomplet\\tnr_clusters\")\n",
    "\n",
    "for i in [2,3,4,5,6,7]:\n",
    "    db = DBSCAN(eps=80, min_samples=i)\n",
    "    tune_db_scan(dbscan=db, value=i, data=train_features, labels=train_labels)\n",
    "\n",
    "print(80 * \"_\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison random, supervisez, dbscan, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247338247338247\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "svm = SVC(kernel= 'linear')\n",
    "svm.fit(train_features, train_labels)\n",
    "y_pred = svm.predict(test_features)\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12285012285012285\n"
     ]
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(train_features, train_labels)\n",
    "y_pred = dummy_clf.predict(test_features)\n",
    "print(accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(algo):\n",
    "\n",
    "\n",
    "    if algo == \"random\":\n",
    "        starting_time = time()\n",
    "        dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "        dummy_clf.fit(train_features, train_labels)\n",
    "        fit_time = time() - starting_time\n",
    "        y_pred_test = dummy_clf.predict(test_features)\n",
    "        y_pred_train = dummy_clf.predict(train_features)\n",
    "\n",
    "        _metrics = [algo,\n",
    "                    fit_time,\n",
    "                    \"none\",\n",
    "                    accuracy_score(train_labels, y_pred_train),\n",
    "                    accuracy_score(test_labels, y_pred_test)]\n",
    "    elif algo == \"SVM\":\n",
    "        starting_time = time()\n",
    "        svm = SVC(kernel= 'linear')\n",
    "        svm.fit(train_features, train_labels)\n",
    "        fit_time = time() - starting_time\n",
    "        y_pred_test = svm.predict(test_features)\n",
    "        y_pred_train = svm.predict(train_features)\n",
    "        _metrics = [algo,\n",
    "                    fit_time,\n",
    "                    \"none\",\n",
    "                    accuracy_score(train_labels, y_pred_train),\n",
    "                    accuracy_score(test_labels, y_pred_test)]\n",
    "    elif algo == \"kmeans\":\n",
    "        starting_time = time()\n",
    "        kmeans = KMeans(init=\"k-means++\", n_clusters=25, n_init=4)\n",
    "        kmeans.fit(train_features)\n",
    "        fit_time = time() - starting_time\n",
    "        labels_mapping = get_labels_mapping(kmeans.labels_,train_labels)\n",
    "\n",
    "        number_labels_train = np.zeros(len(kmeans.labels_))\n",
    "        for i in range(len(kmeans.labels_)):\n",
    "            number_labels_train[i] = labels_mapping[kmeans.labels_[i]]\n",
    "\n",
    "        predicted = kmeans.predict(test_features)\n",
    "        number_labels_test = np.zeros(len(predicted))\n",
    "        for i in range(len(predicted)):\n",
    "            number_labels_test[i] = labels_mapping[predicted[i]]\n",
    "\n",
    "        _metrics =[algo,\n",
    "                  fit_time,\n",
    "                  metrics.silhouette_score(train_features,kmeans.labels_,metric=\"euclidean\"),\n",
    "                  accuracy_score(number_labels_train,train_labels),\n",
    "                  accuracy_score(number_labels_test,test_labels)]\n",
    "\n",
    "    elif algo == \"dbscan\":\n",
    "        starting_time = time()\n",
    "        estimator = make_pipeline(StandardScaler(), DBSCAN(eps=80, min_samples=5)).fit(train_features)\n",
    "        fit_time = time() - starting_time\n",
    "\n",
    "        _metrics = [algo,\n",
    "                    fit_time,\n",
    "                    metrics.silhouette_score(train_features,\n",
    "                                             estimator[-1].labels_),\n",
    "                    \"none\",\n",
    "                    \"none\"]\n",
    "\n",
    "    formatter_result = (\"{}\\t\\t{}\\t{}\\t\\t\\t{}\\t\\t{}\")\n",
    "    print(formatter_result.format(*_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "algorithm\ttime\t\t\tsilhouette\t\tacc_train\t\tacc_test\n",
      "random\t\t0.0010023117065429688\tnone\t\t0.12012012012012012\t0.12285012285012285\n",
      "SVM\t\t34.203367948532104\tnone\t\t0.9997597597597597\t0.8247338247338247\n",
      "kmeans\t\t31.14898657798767\t0.04792613908648491\t\t0.6263063063063063\t0.5831285831285832\n",
      "dbscan\t\t7.996812105178833\t-0.20988327264785767\t\tnone\tnone\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(100 * \"_\")\n",
    "print(\"algorithm\\ttime\\t\\t\\tsilhouette\\t\\t\\tacc_train\\t\\t\\tacc_test\")\n",
    "\n",
    "results(\"random\")\n",
    "results(\"SVM\")\n",
    "results(\"kmeans\")\n",
    "results(\"dbscan\")\n",
    "print(100 * \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________________________________________________\n",
      "algorithm\ttime\t\t\tsilhouette\t\t\tacc_train\t\t\tacc_test\n",
      "random\t\t0.002003908157348633\tnone\t\t\t0.12012012012012012\t\t0.12285012285012285\n",
      "SVM\t\t35.261393785476685\tnone\t\t\t0.9997597597597597\t\t0.8247338247338247\n",
      "kmeans\t\t26.77710747718811\t0.042468223720788956\t\t\t0.6072072072072072\t\t0.579033579033579\n",
      "dbscan\t\t23.04601550102234\t0.40913838148117065\t\t\tnone\t\tnone\n",
      "________________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(120 * \"_\")\n",
    "print(\"algorithm\\ttime\\t\\t\\tsilhouette\\t\\t\\tacc_train\\t\\t\\tacc_test\")\n",
    "\n",
    "results(\"random\")\n",
    "results(\"SVM\")\n",
    "results(\"kmeans\")\n",
    "results(\"dbscan\")\n",
    "print(120 * \"_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PML_Project2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed6e12633e3fc3b3a80a48fdc5e999ffb4f65fc16077e13f163db947d07e608f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
